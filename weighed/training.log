2021-05-12 21:55:32,064 ----------------------------------------------------------------------------------------------------
2021-05-12 21:55:32,064 Model: "WeightedSequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): WordEmbeddings('glove')
    (list_embedding_1): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.05, inplace=False)
        (encoder): Embedding(300, 100)
        (rnn): LSTM(100, 2048)
        (decoder): Linear(in_features=2048, out_features=300, bias=True)
      )
    )
    (list_embedding_2): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.05, inplace=False)
        (encoder): Embedding(300, 100)
        (rnn): LSTM(100, 2048)
        (decoder): Linear(in_features=2048, out_features=300, bias=True)
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=4196, out_features=4196, bias=True)
  (rnn): LSTM(4196, 100, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=200, out_features=20, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-12 21:55:32,064 ----------------------------------------------------------------------------------------------------
2021-05-12 21:55:32,064 Corpus: "Corpus: 14006 train + 1556 dev + 1729 test sentences"
2021-05-12 21:55:32,064 ----------------------------------------------------------------------------------------------------
2021-05-12 21:55:32,064 Parameters:
2021-05-12 21:55:32,064  - learning_rate: "0.1"
2021-05-12 21:55:32,064  - mini_batch_size: "32"
2021-05-12 21:55:32,064  - patience: "3"
2021-05-12 21:55:32,064  - anneal_factor: "0.5"
2021-05-12 21:55:32,064  - max_epochs: "1"
2021-05-12 21:55:32,064  - shuffle: "True"
2021-05-12 21:55:32,064  - train_with_dev: "True"
2021-05-12 21:55:32,065  - batch_growth_annealing: "False"
2021-05-12 21:55:32,065 ----------------------------------------------------------------------------------------------------
2021-05-12 21:55:32,065 Model training base path: "model/weighed"
2021-05-12 21:55:32,065 ----------------------------------------------------------------------------------------------------
2021-05-12 21:55:32,065 Device: cuda:0
2021-05-12 21:55:32,065 ----------------------------------------------------------------------------------------------------
2021-05-12 21:55:32,065 Embeddings storage mode: cpu
2021-05-12 21:55:32,065 ----------------------------------------------------------------------------------------------------
2021-05-12 21:55:45,738 epoch 1 - iter 48/487 - loss 11.16323508 - samples/sec: 112.36 - lr: 0.100000
2021-05-12 21:55:58,811 epoch 1 - iter 96/487 - loss 7.84683536 - samples/sec: 117.51 - lr: 0.100000
2021-05-12 21:56:08,737 epoch 1 - iter 144/487 - loss 6.21110558 - samples/sec: 154.78 - lr: 0.100000
2021-05-12 21:56:21,695 epoch 1 - iter 192/487 - loss 5.36909229 - samples/sec: 118.55 - lr: 0.100000
2021-05-12 21:56:36,358 epoch 1 - iter 240/487 - loss 4.86749910 - samples/sec: 104.77 - lr: 0.100000
2021-05-12 21:56:50,470 epoch 1 - iter 288/487 - loss 4.53086995 - samples/sec: 108.86 - lr: 0.100000
2021-05-12 21:57:05,924 epoch 1 - iter 336/487 - loss 4.23292763 - samples/sec: 99.41 - lr: 0.100000
2021-05-12 21:57:20,264 epoch 1 - iter 384/487 - loss 3.96261959 - samples/sec: 107.12 - lr: 0.100000
2021-05-12 21:57:34,814 epoch 1 - iter 432/487 - loss 3.76324776 - samples/sec: 105.58 - lr: 0.100000
2021-05-12 21:57:51,615 epoch 1 - iter 480/487 - loss 3.57515283 - samples/sec: 91.44 - lr: 0.100000
2021-05-12 21:57:53,755 ----------------------------------------------------------------------------------------------------
2021-05-12 21:57:53,755 EPOCH 1 done: loss 3.5539 - lr 0.1000000
2021-05-12 21:58:07,324 TEST : loss 1.0069849491119385 - score 0.8838
2021-05-12 21:58:07,406 BAD EPOCHS (no improvement): 0
2021-05-12 21:58:09,736 ----------------------------------------------------------------------------------------------------
2021-05-12 21:58:09,736 Testing using best model ...
2021-05-12 21:58:13,803 0.8841	0.8835	0.8838
2021-05-12 21:58:13,803 
Results:
- F1-score (micro) 0.8838
- F1-score (macro) 0.8640

By class:
LOC        tp: 859 - fp: 116 - fn: 34 - precision: 0.8810 - recall: 0.9619 - f1-score: 0.9197
MISC       tp: 312 - fp: 79 - fn: 124 - precision: 0.7980 - recall: 0.7156 - f1-score: 0.7545
ORG        tp: 594 - fp: 89 - fn: 145 - precision: 0.8697 - recall: 0.8038 - f1-score: 0.8354
PER        tp: 822 - fp: 55 - fn: 38 - precision: 0.9373 - recall: 0.9558 - f1-score: 0.9465
2021-05-12 21:58:13,803 ----------------------------------------------------------------------------------------------------
